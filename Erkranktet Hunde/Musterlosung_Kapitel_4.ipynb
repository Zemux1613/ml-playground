{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial_1 Das BWKI Hundetutorial üê∂ü©∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musterl√∂sung f√ºr Kapitel 4 üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist die Musterl√∂sung zu den Programmieraufgaben des vierten Kapitels - einem sehr mathematischen Kapitel.\n",
    "\n",
    "Schaut sie euch erst an wenn ihr versucht habt, die √úbung selbstst√§ndig zu l√∂sen und nicht mehr weiter kommt. Bei Fragen k√∂nnt ihr euch jederzeit auf unserem Discord Server mit anderen TeilnehmerInnen austauschen!\n",
    "\n",
    "Viel Erfolg bei diesem Kapitel!\n",
    "\n",
    "Als erstes m√ºssen wir unsere **Daten einlesen** und f√ºr die Datenverarbeitung ben√∂tigte Packages (Module) importieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereitung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importiere numpy, ein Paket zur Datenverarbeitung\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/ML/Erkranktet Hunde'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pr√ºfe in welchem Arbeitsverzeichnis (working directory) du gerade arbeitest \n",
    "#Im selben Ordner m√ºssen auch die Daten gespeicherts sein\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Du bist noch nicht im richtigen Ordner? √Ñndere mit cd / deine working directory\n",
    "cd /Users/...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen der Daten\n",
    "D = np.load('train_data.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eigenschaften des Datensatzes einsehen (Zeilen, Spalten)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was bisher geschah ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den vorangegangenen Kapiteln hast du gelernt einzelne Datenpunkte aus einem Datensatz zu extrahieren, kannst for und while Schleifen, if-Abfragen und Funktionen der numpy-Bibliothek anwenden. Zus√§tzlich hast du gelernt, wie du einen Klassifikator mit ein oder mehreren Merkmalen implementierst und wie du √ºberpr√ºfst, ob der Klassifikator eine hohe Genauigkeit f√ºr seine Zuordnungen aufweist. In den folgenden Aufgaben wirst du lernen, wie dein Algorithmus von selbst herausfindet welche Gewichte zum besten Ergebnis f√ºhren - d.h. kein Ausprobieren mehr, sondern die Anwendung des **Gradientenverfahren**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der ersten √úbung ist es an der Zeit eine Kostenfunktion, auf Basis des nat√ºrlichen Logarithmus, zu implementieren. Die Kostenfunktion gibt dir ein Feedback, wie \"schlimm\" die Fehler in den Vorhersagen unseres Algorithmus waren. <br> **Wichtig:** Die Kostenfunktion gibt einen kleinen Wert zur√ºck, wenn unsere Vorhersage (fast) richtig ist und einen gro√üen Wert, wenn sie (sehr) daneben liegt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deine Aufgabe ist es die Kostenfunktion zu implementieren und den Mittelwert aller Kosten zur√ºck zu geben (Abschnitt 5/15). Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "#definiere cost = Kostenfunktion\n",
    "def cost(predictions, labels):\n",
    "    \"\"\"Kostenfunktion.\n",
    "    \n",
    "    Args:\n",
    "        predictions (numpy.ndarray): Array mit den Wahrscheinlichkeiten f√ºr jeden Hund\n",
    "        labels (numpy.ndarray): Array mit den Labels (0 oder 1) f√ºr jeden Hund\n",
    "    Returns:\n",
    "        float: je kleiner der Wert desto besser ist die Vorhersage des Klassifikators\n",
    "    \"\"\"\n",
    "    # Abfangen eines Sonderfalls: Der Logarithmus ist an der Stelle 0 nicht definiert.\n",
    "    epsilon = 1e-12\n",
    "    predictions[predictions <= epsilon] = epsilon\n",
    "    predictions[predictions >= 1. - epsilon] = 1. - epsilon\n",
    "\n",
    "    cost = np.mean(-(labels*np.log(predictions)\n",
    "             + (1-labels)*np.log(1-predictions)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super - Aufgabe gel√∂st!\n",
    "Die obige Funktion ist vorerst ein Bauplan, ein Test ob diese funktioniert ist jedoch mit ein paar Tricks m√∂glich!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipps & Tricks (nicht f√ºr L√∂sung im KI Kurs notwendig)\n",
    "Wir k√∂nnen cost zwar nicht \"einfach so\" testen, aber da das ja eine ganz allgemeine Kostenfunktion ist,\n",
    "k√∂nnen wir uns f√ºr den Test tempor√§r eigene Daten in einem numpy.array definieren & cost auf diesen Daten ausf√ºhren.\n",
    "Das geht z.B. so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kosten der schlechten Vorhersagen: 7.7002766941568295\n",
      "Kosten der guten Vorhersagen: 0.0790203867436197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unser tempor√§rer Datensatz\n",
    "labels_temporary = np.array([1.0,\n",
    "                            0.0,\n",
    "                            1.0,\n",
    "                            0.0])\n",
    "\n",
    "#Wir denken uns einige stark abweichende Vorhersagen aus\n",
    "preds_bad_temporary = np.array([0.7,\n",
    "                               0.4,\n",
    "                               0.0,\n",
    "                               0.9])\n",
    "\n",
    "#Wir denken uns einige gut passende Vorhersagen aus\n",
    "preds_good_temporary = np.array([0.9,\n",
    "                                0.1,\n",
    "                                0.9,\n",
    "                                0.0])\n",
    "\n",
    "#Wir wenden die Kostenfunktion an und berechnen wie ung√ºnstig die Fehler in der Vorhersage sind \n",
    "cost_bad = cost(preds_bad_temporary,labels_temporary)\n",
    "cost_good = cost(preds_good_temporary,labels_temporary)\n",
    "\n",
    "#Wir lassen uns das Ergebnis ausgeben\n",
    "print(\"Kosten der schlechten Vorhersagen: {}\\nKosten der guten Vorhersagen: {}\\n\".format(cost_bad,cost_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies sind komplett zuf√§llig ausgedachte Daten, die wir einfach nur zu Testzwecken verwenden.\n",
    "Wie wir sehen, gibt die Kostenfunktion einen h√∂heren Wert zur√ºck,\n",
    "wenn unsere Vorhersagen weiter weg von den tats√§chlichen Labels der Originaldaten liegen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu berechnen wann die Kostenfunktion minimal ist, wenden wir das Gradientenverfahren an. Um das Minimum zu bestimmen, verwenden wir die partielle Ableitung. Deine Aufgabe ist es die Funktion derivative, die die partielle Ableitung der Kostenfunktion nach den Gewichten w1, w2 und dem Bias b berechnet, zu implementieren und die Funktion p() anzupassen (Abschnitt 11/15). Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "#Zuerst m√ºssen wir einige Funktionen implementieren, die wir schon aus den vorherigen Kapiteln kennen\n",
    "\n",
    "# Um die Wahrscheinlichkeit f√ºr eine Zuordnung zu berechnen (Wie sicher ist sich der Algorithmus?)\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid Funktion\n",
    "    Args:\n",
    "        x (np.ndmatrix): Werte auf den die Sigmoid-Funktion angewendet wird\n",
    "    Returns:\n",
    "         np.array: Transformierte Werte.\n",
    "    Note:\n",
    "        Verwende die np.exp Funktion. Alle Funktionen die du hier verwendest\n",
    "        aktzeptieren auch Vektoren. Somit unterscheidet sich die\n",
    "        Vektor-Implementierung nicht von der Skalaren version.\n",
    "    \"\"\"\n",
    "    s = 1./(1. + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "#Um die Wahrscheinlichkeit f√ºr alle Zuordnungen zu berechnen, unter Ber√ºcksichtigung von 2 Merkmalen\n",
    "def p(X, w1, w2, b):\n",
    "    \"\"\"Wahrscheinlichkeit dass ein Hunde krank ist fuer alle Hunde gleichzeitig.\n",
    "    Args:\n",
    "        X (numpy.ndarray): Datenmatrix:\n",
    "            - pro Hund eine Zeile (erster Index)\n",
    "            - pro Eigenschaft eine Spalte (zweiter Index)\n",
    "        w1 (float): Gewichtet die Herzfrequenz.\n",
    "        w2 (float): Gewichtet die ST-Depression\n",
    "        b (float): Verschiebung zur Unterscheidung des Gesundheitszustands.\n",
    "        Returns:\n",
    "            numpy.ndarray: Vektor:\n",
    "                - pro Hund eine Wahrscheinlichkeit, dass der Hund krank ist\n",
    "                    (Werte < 0.5: Hund ist wahrscheinlich gesund,\n",
    "                     Werte > 0.5 Hund ist wahrcheinlich krank.)\n",
    "    \"\"\"\n",
    "    l = X[:, 0]*w1 + X[:, 1]*w2 + b\n",
    "    v = sigmoid(l)\n",
    "    return v\n",
    "\n",
    "#Um das Minimum zu berechnen\n",
    "def derivative(D, w1, w2, b):\n",
    "    \"\"\"Ableitung der Gewichte und des Bias.\n",
    "    Args:\n",
    "        D (numpy.ndarray): Datenmatrix:\n",
    "            - pro Hund eine Zeile (erster Index)\n",
    "            - pro Eigenschaft eine Spalte (zweiter Index)\n",
    "        w1 (float): Gewichtet die Herzfrequenz.\n",
    "        w2 (float): Gewichtet die ST-Depression\n",
    "        b (float): Verschiebung zur Unterscheidung des Gesundheitszustands.\n",
    "        Returns:\n",
    "            (float, float, float): Die neuen Gweichte (w1, w2) und den neuen Bias (b).\n",
    "    \"\"\"\n",
    "\n",
    "    labels = D[:, -1]\n",
    "    X = D[:, [7, 9]]\n",
    "\n",
    "    d_w1 = np.mean((p(X, w1, w2, b) - labels) * X[:, 0])\n",
    "    d_w2 = np.mean((p(X, w1, w2, b) - labels) * X[:, 1])\n",
    "    d_b = np.mean((p(X, w1, w2, b) - labels))\n",
    "    return d_w1, d_w2, d_b\n",
    "\n",
    "#Um einen Vergleich zw. Vorhersage und den vorliegenden Datenpunkten anzustellen\n",
    "def accuracy(Y, Y_hat):\n",
    "    \"\"\"Accuracy der Predictions.\n",
    "    Args:\n",
    "        Y (numpy.ndarray): Vektor mit den Lables:\n",
    "            - pro Hund ein Wert mit dem tatsaechlichem\n",
    "                gesundheitszustand des Hundes.\n",
    "        Y_hat (numpy.ndarray): Vektor mit Predictions (0 oder 1) f√ºr jeden Hund.\n",
    "        Returns:\n",
    "            (float): Accuracy.\n",
    "    \"\"\"\n",
    "    return np.mean(Y == Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipps & Tricks (nicht f√ºr L√∂sung im KI Kurs notwendig)\n",
    "\n",
    "Erneut k√∂nnen wir die Funktionen nicht \"einfach so\" testen, aber wir k√∂nnen uns f√ºr den Test wieder tempor√§r eigene Daten ausdenken und diese in die Funktionen einsetzen. Somit erhalten wir einen ersten Eindruck √ºber den Output unserer Funktionen. Das geht z.B. so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5397868702434395e-05 0.5124973964842103 0.9999546021312976\n"
     ]
    }
   ],
   "source": [
    "#Sigmoid\n",
    "print(sigmoid(-10), sigmoid(0.05), sigmoid(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82244483, 0.75119578, 0.79017822, 0.87157983, 0.85119443,\n",
       "       0.8579053 , 0.83834971, 0.83834971, 0.81667789, 0.83075675])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p\n",
    "X = D[:, [7, 9]]\n",
    "p(X,0.01,0.01,0.01)[:10] #zeige nur die ersten 10 Eintr√§ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.99401820361357, 0.052416868246057534, 0.20793499726578424)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#derivative\n",
    "derivative(D,0.01,0.01,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bsp. 1:\n",
      "Genauigkeit der schlechten Vorhersagen: 0.5\n",
      " Genauigkeit der guten Vorhersagen: 1.0\n",
      "\n",
      "--------------------\n",
      "Bsp.2:\n",
      "Genauigkeit der Vorhersagen: 0.596244131455399\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "\n",
    "#1. imagin√§res Beispiel (identisch zu oben)\n",
    "preds_bad_temporary = np.array([0.7,\n",
    "                               0.4,\n",
    "                               0.0,\n",
    "                               0.9])\n",
    "\n",
    "preds_good_temporary = np.array([0.9,\n",
    "                                0.1,\n",
    "                                0.9,\n",
    "                                0.0])\n",
    "\n",
    "# wichtig: aufrunden!\n",
    "preds_bad_temporary = np.round(preds_bad_temporary)\n",
    "preds_good_temporary = np.round(preds_good_temporary)\n",
    "\n",
    "labels_temporary = np.array([1.0,\n",
    "                            0.0,\n",
    "                            1.0,\n",
    "                            0.0])\n",
    "\n",
    "acc_bad = accuracy(preds_bad_temporary.round(),labels_temporary)\n",
    "acc_good = accuracy(preds_good_temporary,labels_temporary)\n",
    "\n",
    "#Zur verbesserten √úbersicht lassen wir uns zu unserer Ausgabe eine Zuordnung (Bsp.1) mit ausgeben\n",
    "print(\"Bsp. 1:\")\n",
    "#Zum besseren und direkten Verst√§ndnis\n",
    "print(\"Genauigkeit der schlechten Vorhersagen: {}\\n Genauigkeit der guten Vorhersagen: {}\\n\".format(acc_bad,acc_good))\n",
    "#Zur besseren Strukturierung unserer Ausgabe lassen wir uns eine gestrichelte Linie ausgeben\n",
    "print(\"-\"*20)\n",
    "\n",
    "#2. Beispiel mit obiger Funktion p\n",
    "\n",
    "preds = np.round(p(X,0.01,0.01,0.01)) #Runden nicht vergessen!\n",
    "labels = D[:, -1]\n",
    "acc = accuracy(preds,labels)\n",
    "\n",
    "print(\"Bsp.2:\")\n",
    "print(\"Genauigkeit der Vorhersagen: {}\".format(acc))\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super jetzt hast du getestet, dass die von dir gecodeten Funktionen klappen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deine Aufgabe ist es mittels des Gradientenverfahrens die Gewichte und den Bias zu finden, welche uns die besten Diagnosen liefern. Damit wir kein globales Minimum √ºbersehen (zu gro√üe Lernrate) oder gar nicht erst zu einem globalen Minimum kommen (zu kleine Lernrate), m√ºssen wir die √Ñnderungen unserer Parameter schrittweise, mittels der **Step-Funktion** vornehmen. Implementiere die Step-Funktion (13/15). Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "#Sigmoid Funktion - wurde berits oben definiert\n",
    "#def sigmoid(x):\n",
    "    #s = 1./(1. + np.exp(-x))\n",
    "    #return s\n",
    "\n",
    "#Wahrscheinlichkeit - wurde berits oben definiert\n",
    "#def p(X, w1, w2, b):\n",
    "    #l = X[:, 0]*w1 + X[:, 1]*w2 + b\n",
    "    #v = sigmoid(l)\n",
    "    #return v\n",
    "\n",
    "#Fkt. derivative - wurde bereits oben implementiert, wird jedoch um labels erweitert    \n",
    "def derivative(X, labels, w1, w2, b):\n",
    "    d_w1 = np.mean((p(X, w1, w2, b) - labels) * X[:, 0])\n",
    "    d_w2 = np.mean((p(X, w1, w2, b) - labels) * X[:, 1])\n",
    "    d_b = np.mean((p(X, w1, w2, b) - labels))\n",
    "    return d_w1, d_w2, d_b\n",
    "    \n",
    "    labels = D[:, -1]\n",
    "    X = D[:, [7, 9]]\n",
    "\n",
    "\n",
    "#Genauigkeit - wurde beireits im letzten Schritt implementiert\n",
    "#def accuracy(Y, Y_hat):\n",
    "    #return np.mean(Y == Y_hat)\n",
    "\n",
    "def step(D, w1, w2, b, alpha):\n",
    "    \"\"\"Durchfuehren eines Optimierungsschritts.\n",
    "    Args:\n",
    "        D (numpy.ndarray): Datenmatrix:\n",
    "            - pro Hund eine Zeile (erster Index)\n",
    "            - pro Eigenschaft eine Spalte (zweiter Index)\n",
    "        w1 (float): Gewichtet die Herzfrequenz.\n",
    "        w2 (float): Gewichtet die ST-Depression\n",
    "        b (float): Verschiebung zur Unterscheidung des Gesundheitszustands.\n",
    "        alpha (float): Learning rate.\n",
    "        Returns:\n",
    "            (float, float, float): Die neuen Gewichte (w1, w2) und den neuen Bias (b).\n",
    "    \"\"\"\n",
    "    labels = D[:, -1]\n",
    "    X = D[:, [7, 9]]\n",
    "\n",
    "    d_w1, d_w2, d_b = derivative(D, labels, w1, w2, b)\n",
    "                                 \n",
    "    w1_neu = w1 - alpha * d_w1\n",
    "    w2_neu = w2 - alpha * d_w2\n",
    "    b_neu = b - alpha * d_b\n",
    "    return w1_neu, w2_neu, b_neu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipps & Tricks (nicht f√ºr L√∂sung im KI Kurs notwendig)\n",
    "\n",
    "Sch√∂n und gut, die Step-Funktion ist nun implementiert. Aber wie k√∂nnen wir uns anschauen, was sie bewirkt und ob wir alles richtig gemacht haben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.09983921595658289, 0.09999402189422861, 0.09998322876865283)\n"
     ]
    }
   ],
   "source": [
    "#Teste deine Implementierung\n",
    "#Setze im ersten Schritt alle Gewichte und den Bias auf 0.1 sowie die Lernrate auf alpha=0.0001\n",
    "print(step(D,0.1, 0.1,0.1,0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Optimierungsschritt wurde durchgef√ºhrt! Die Ausgabe zeigt dir die neu gesch√§tzten Gewichte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die n√§chste Aufgabe des 4. Kapitels (Abschnitt 14/15), werden wir schrittweise bearbeiten. Wir starten mit Schritt 1, bei dem wir die bestehende Modellfunktion p() an ein multivariates-Modell anpassen m√ºssen. Das bedeutet, dass jetzt alle 13 Merkmale verwendet werden, um den Gesunheitszustand eines Hundes (14. Merkmal) vorherzusagen. Es entstehen Gewichts- und Merkmalsvektoren, f√ºr deren Verrechnung wir wieder die numpy Funktion np.dot verwenden (Reminder - Skalarprodukt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1./(1. + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def p(X, w):\n",
    "    l = np.dot(X, w)\n",
    "    v = sigmoid(l)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schritt 2. (Abschnitt 14/15) nun passen wir auch das Gradientenabstiegsverfahren f√ºr alle Merkmale an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(X, labels, w):\n",
    "    d = np.mean((p(X, w) - labels)[:, None] * X, axis=0)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Schritt 3(a) & 3(b) des (Abschnitt 14/15), erweitern wir nun auch die Step Funktion auf alle Merkmale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a erweitern der Step Funktion\n",
    "def step(D, w, alpha):\n",
    "    labels = D[:, -1]\n",
    "    X = np.ones((D.shape[0], D.shape[1]))\n",
    "    \n",
    "    X[:, :-1] = D[:, :-1] #Erkl√§rung\n",
    "    \n",
    "    d_w = derivative(X, labels, w) #Erkl√§rung\n",
    "    \n",
    "    w_neu = w - alpha * d_w #3b Gewichtsvektor aktualisieren\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der letzten Aufgabe des vierten Kapitels geht es um die Optimierung, also die Steigerung der Genauigkeit unseres Multivariaten Modells (Abschnitt 15/15). Wir werden die Funktion optimierung() implementieren, die unsere Parameter (Gewichte, Bias) so lange aktualisiert, bis sich unsere Vorhersage nicht mehr verbessert. Dies k√∂nnen wir mit der Implementierung einer for-Schleife bewirken, die abbricht (break) sofern der Unterschied sehr gering ausf√§llt z.B. < 1eÀÜ-6 ist. Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1./(1. + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def p(X, w):\n",
    "    l = np.dot(X, w)\n",
    "    v = sigmoid(l)\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def derivative(X, Y, w):\n",
    "    d = np.mean((p(X, w) - Y)[:, None] * X, axis=0)\n",
    "    return d\n",
    "\n",
    "\n",
    "def accuracy(Y, Y_hat):\n",
    "    return np.mean(Y == Y_hat)\n",
    "\n",
    "def cost(predictions, labels):\n",
    "    \n",
    "    eps = np.finfo(float).eps # avoid dividing by zero in log\n",
    "    \n",
    "    predictions[predictions==0] = 1e-16\n",
    "    predictions[predictions==1] = 1.-1e-16   \n",
    "    c = np.mean(-(labels*np.log(predictions)\n",
    "          + (1-labels)*np.log(1-predictions + eps)))\n",
    "    return c\n",
    "\n",
    "def step(D, w, alpha):\n",
    "    X = np.ones((D.shape[0], D.shape[1]))\n",
    "    X[:, 1:] = D[:, :-1]\n",
    "    Y = D[:, -1]\n",
    "    \n",
    "    labels = D[:, -1]\n",
    "    d_w = derivative(X, Y, w)\n",
    "    w_new = w - alpha * d_w\n",
    "    return w_new\n",
    "\n",
    "def vorhersage(X, w):\n",
    "    Y_hat = np.round((p(X, w) > 0.5))\n",
    "    return Y_hat\n",
    "\n",
    "def optimierung(D):\n",
    "    w = np.random.normal(size=14)*0.01\n",
    "    alpha = 0.0001\n",
    "\n",
    "    X = np.ones((D.shape[0], D.shape[1]))\n",
    "    X[:, 1:] = D[:, :-1]\n",
    "\n",
    "    last_cost = np.inf\n",
    "    for i in range(50000):\n",
    "        w = step(D, w=w, alpha=alpha)\n",
    "        current_cost = cost(p(X, w), D[:, -1])\n",
    "        if (np.abs(last_cost - current_cost) < 1e-6):\n",
    "            break\n",
    "        last_cost = current_cost\n",
    "    Y_hat = vorhersage(X, w)\n",
    "    print(\"DONE\", i, accuracy(D[:, -1], Y_hat))\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puuuuh - Im Fenster des KI Kurses wird keine Fehlermeldung angezeigt. Yeei! ... Aber was genau bringt uns dieser lange Code-Block? Im n√§chsten Abschnitt testen wir unser Multivariates Modell und schauen uns an, wie genau die Vorhersage unseres Modells mittels der Optimierungsfunktion werden kann. Bereit ?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipps & Tricks (nicht f√ºr L√∂sung im KI Kurs notwendig)\n",
    "\n",
    "### Ein Multivariates Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir unser Modell Schritt f√ºr Schritt erweitert und sind bei einem multivariaten Modell angelangt, welches alle 13 Variablen in die Vorhersage von gesund oder krank mit einbezieht. Zus√§tzlich haben wir im letzten Schritt eine Optimierungsfunktion eingebaut, damit die Vorhersagen unseres Modells so genau wie m√∂glich sind. \n",
    "\n",
    "<br> Jetzt wollen wir das Ganze nat√ºrlich auch mal testen und schauen, welche Genauigkeit unser Modell erzielen kann. Dieser Teil ist **nicht f√ºr die L√∂sung des KI Kurses n√∂tig**, gibt dir allerdings ein besseres Verst√§ndnis f√ºr die Funktionen und dein multivariates Modell. Wenn du noch nicht alle Schritte im Code verstehst, macht das √ºberhaupt nichts! In Kapitel 5 gehen wir fundierter auf das Trainieren von komplexen Modellen ein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion optimierung() erledigt den Gro√üteil der Arbeit f√ºr uns. Schaut euch die #Kommentare an, damit ihr jeden einzelnen Schritt nachverfolgen k√∂nnt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.        ,  1.        ,  1.        , ...,  3.        ,\n",
       "         0.        ,  6.        ],\n",
       "       [14.33333333,  1.        ,  4.        , ...,  2.        ,\n",
       "         3.        ,  3.        ],\n",
       "       [14.33333333,  1.        ,  4.        , ...,  2.        ,\n",
       "         2.        ,  7.        ],\n",
       "       ...,\n",
       "       [16.        ,  1.        ,  3.        , ...,  2.        ,\n",
       "         2.        ,  5.78873239],\n",
       "       [16.33333333,  0.        ,  3.        , ...,  1.        ,\n",
       "         1.6       ,  3.        ],\n",
       "       [16.66666667,  1.        ,  2.        , ...,  1.        ,\n",
       "         1.6       ,  5.78873239]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wir werfen erst mal einen Blick in unseren Datensatz\n",
    "D[:, :-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((D.shape[0], D.shape[1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimierung(D):\n",
    "    w = np.random.normal(size=14)*0.01 # Initialisierung der Gewichte\n",
    "    alpha = 0.0001 # Festlegen der Lernrate\n",
    "\n",
    "    X = np.ones((D.shape[0], D.shape[1])) # wir generieren ein Array mit der gleichen Gr√∂√üe wie D und f√ºllen es mit 1en auf\n",
    "    X[:, 1:] = D[:, :-1] # wir √ºbernehmen alle Werte bis auf die Labels (die soll unser Klassifikator ja nicht sehen) in unser neues Array\n",
    "    Y = D[:, -1]\n",
    "    \n",
    "    last_cost = np.inf # optimale Kosten tracken\n",
    "    for i in range(50000): #50000 Epochen iterieren lassen\n",
    "        w_neu = step(D, w=w, alpha=alpha) # einen Trainingsschritt ausf√ºhren\n",
    "        w = w_neu\n",
    "        \n",
    "        current_cost = cost(p(X, w), D[:, -1])  # aktuelle Kosten berechnen\n",
    "        if (np.abs(last_cost - current_cost) < 1e-6): # wenn Kosten nicht mehr sinken -> abbrechen (early stopping)\n",
    "            break\n",
    "        last_cost = current_cost # Kosten aktualisieren (nur zum Tracken)\n",
    "    Y_hat = vorhersage(X, w)  # ganz am Ende Vorhersage machen\n",
    "    print(\"DONE\", i, accuracy(Y, Y_hat)) #Ausgabe\n",
    "    return Y_hat # Vorhersage zur√ºckgeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Optimierungs Funktion trainiert unser Modell bis sich unsere Vorhersage nicht mehr verbessert. Am Ende speichern wir die Vorhersagen in der Variable preds ab. Alles was wir dann noch tun m√ºssen ist, unsere Kosten sowie die Genauigkeit zu berechnen und hierf√ºr m√ºssen wir preds mit den labels (Originaldaten) vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE 31153 0.8403755868544601\n"
     ]
    }
   ],
   "source": [
    "#Abspeichern der Variablen, um diese der Kostenfunktion zu √ºbergeben\n",
    "preds = optimierung(D)\n",
    "labels =  D[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8403755868544601"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = D[:, -1]\n",
    "accuracy(preds,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Modell erzielt eine Genauigkeit von 84%? Na wer sagts denn? :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis**: Du erh√§lst eine Fehlermeldung?\n",
    "    Aktuell liefert uns unsere vorhersage-Funktion booleans, also Wahrheitswerte zur√ºck.\n",
    "    Diese werden von unserer Kostenfunktion nicht richtig interpretiert und deshalb tritt hier eine Fehlermeldung auf.\n",
    "    Wir k√∂nnen diesen Fehler beheben, indem wir unsere vorhersage-Funktion anpassen und Gleitkommazahlen statt Booleans\n",
    "    zur√ºckgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vorhersage(X, w):\n",
    "    Y_hat = (p(X, w) > 0.5)\n",
    "    return Y_hat.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun tritt auch keine Fehlermeldung auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100/1499381281.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "  c = np.mean(-(labels*np.log(predictions)\n",
      "/tmp/ipykernel_100/1499381281.py:29: RuntimeWarning: invalid value encountered in multiply\n",
      "  c = np.mean(-(labels*np.log(predictions)\n",
      "/tmp/ipykernel_100/1499381281.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  + (1-labels)*np.log(1-predictions + eps)))\n",
      "/tmp/ipykernel_100/1499381281.py:30: RuntimeWarning: invalid value encountered in multiply\n",
      "  + (1-labels)*np.log(1-predictions + eps)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8403755868544601"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Du hast es geschafft! ü•≥ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch das vierte Kapitel haben wir erfolgreich √úberstanden! üí™üèº Das war mit Abstand das mathematischste Kapitel dieses Tutorials - Super gemacht! <br> Mach eine kurze oder l√§ngere Pause! <br> Denn ... \n",
    "jetzt geht es daran ein neuronales Netz zu programmieren ... üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
