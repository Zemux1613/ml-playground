{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9ReR2s-jISg"
   },
   "source": [
    "# Tutorial_1 Das BWKI Hundetutorial üê∂ü©∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJZRh3G3jIS6"
   },
   "source": [
    "## Musterl√∂sung f√ºr Kapitel 5 üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdRmnxKvjIS_"
   },
   "source": [
    "Dies ist die Musterl√∂sung zu den Programmieraufgaben des f√ºnften & letzten Kapitels. \n",
    "\n",
    "Schaut sie euch erst an wenn ihr versucht habt, die √úbung selbstst√§ndig zu l√∂sen und nicht weiter kommt. Bei Fragen k√∂nnt ihr euch jederzeit auf unserem Discord Server mit anderen Teilnehmer:innen austauschen!\n",
    "\n",
    "Viel Erfolg bei diesem Tutorial!\n",
    "\n",
    "Als erstes m√ºssen wir unsere **Daten einlesen** und f√ºr die Datenverarbeitung ben√∂tigte Packages (Module) einlesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqz4EHZBjITG"
   },
   "source": [
    "### Vorbereitung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IZ6JsVIRjITL"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Importiere numpy, ein Paket zur Datenverarbeitung\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#Importiere numpy, ein Paket zur Datenverarbeitung\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1634541155704,
     "user": {
      "displayName": "Mc Toel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giium7m5VTeymW5dJqmq-Vk873KunGLP04z1EeM=s64",
      "userId": "09970438854856090746"
     },
     "user_tz": -120
    },
    "id": "qhi1YqaZjITV",
    "outputId": "056cca31-f636-4255-a52e-430f552b8750"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/ML/Erkranktet Hunde'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pr√ºfe in welchem Arbeitsverzeichnis (working directory) du gerade arbeitest \n",
    "#Im selben Ordner m√ºssen auch die Daten gespeicherts sein\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "prqRUnpwjITj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Du bist noch nicht im richtigen Ordner? √Ñndere mit cd /Pfad deine working directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcd\u001b[49m \u001b[38;5;241m/\u001b[39mUsers\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cd' is not defined"
     ]
    }
   ],
   "source": [
    "#Du bist noch nicht im richtigen Ordner? √Ñndere mit cd /Pfad deine working directory\n",
    "cd /Users/ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K1Rvu7HrjITm"
   },
   "outputs": [],
   "source": [
    "#Einlesen der Daten\n",
    "# auf Windows: /\n",
    "# auf Mac & Linux: \\\n",
    "D = np.load('train_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hXn_JXtYjITr",
    "outputId": "008dd984-ddd4-43b8-9f08-b438c80cb89d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eigenschaften des Datensatzes einsehen (Zeilen, Spalten)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTL4tJSAjITz"
   },
   "source": [
    "### Was bisher geschah ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTTVbDlIjIT2"
   },
   "source": [
    "... eine ganze Menge !! In den vorangegangenen Kapiteln hast du gelernt, einzelne Datenpunkte aus einem Datensatz zu extrahieren for- und while-Schleifen, if-Abfragen sowie Funktionen der numpy-Bibliothek anzuwenden. Du hast gelernt, wie du einen Klassifikator mit ein oder mehreren Merkmalen implementierst und wie man einen solchen Klassifikator bewertet, indem man die Genauigkeit des Klassifikators auf einem Testdatensatz berechnet. Ebenso hast du gelernt, wie du die Gewichte und das Bias mittels des Gradientenabstiegverfahrens bestimmst. Ein Modell mit wenigen Merkmalen ist einem Modell mit multiplen Merkmalen in der Geauigkeit seiner Vorhersagen i.d.R. unterlegen, weshalb du multivariate Modelle kennengelernt hast.\n",
    "\n",
    "Im letzten Kapitel lernst du nun wie man neuronale Netze (abgek√ºrzt NN) implementieren, trainieren, und anschlie√üend evaluieren kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO9591R8jIT7"
   },
   "source": [
    "In der ersten √úbung des 5. Kapitels musst du deinen Datensatz in einen Trainings- und einen Testdatensatz aufteilen. Dadurch kannst du anschlie√üend die Genauigkeit deines neuronalen Netzes evaluieren indem du dein Netz auf neuen Daten testest (Abschnitt 6/11). Nimm 80% aller Daten f√ºr das Training und die restlichen 20% als Testdatensatz. Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3sO4UU-ZjIT-"
   },
   "outputs": [],
   "source": [
    "def train_test_split(D):\n",
    "    \n",
    "    train_ix = np.random.random(size=D.shape[0]) < 0.8\n",
    "    test_ix = np.logical_not(train_ix)\n",
    "    \n",
    "    train_data = D[train_ix]\n",
    "    test_data = D[test_ix]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "isTYy2JFjIUC",
    "outputId": "79856c1e-139e-4329-8ece-0f3ceb28aa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension der Trainingsdaten: (343, 14), Dimension der Testdaten: (83, 14)\n"
     ]
    }
   ],
   "source": [
    "# nicht f√ºr die L√∂sung im KI-Kurs notwendig\n",
    "# Eigenschaften der 2 verschiedenen Datens√§tze einsehen (Zeilen, Spalten)\n",
    "train_data, test_data = train_test_split(D)\n",
    "print(f\"Dimension der Trainingsdaten: {train_data.shape}, Dimension der Testdaten: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDXlSQS6jIUG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x27IyG8njIUI"
   },
   "source": [
    "Wir k√∂nnen die logistische Regession, die wir in den vorangegangenen Aufgaben implementiert haben, auch mit Pytorch realisieren. Hierf√ºr verwenden wir die `nn.Linear-Klasse`. \n",
    "Indem wir ein \"neuronales Netz\" implementieren, welches nur eine Schicht hat, k√∂nnen wir eine logistische Regression simulieren. Im weiteren Verlauf des Tutorials werden wir nat√ºrlich auch noch komplexere Netze bauen! Die Kostenfunktion - hier nennen wir sie loss - ist in `nn.BCEWithLogitsLos` implementiert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNQKWwkvjIUK"
   },
   "source": [
    "Unser neuronales Netz besteht in diesem Fall aus einer einzigen Schicht mit 13 Neuronen, welche direkt mit den Inputs verbunden ist. D.h. die Ausgabe unseres Netzes ist eine einfache gewichtete Summe der Inputs. Finde passende Werte f√ºr die Hyperparameter `n_step` (Anzahl der Trainingsdurchg√§nge) und `learning_rate` (auch alpha genannt). Bestimme wie stark sich das Netzwerk w√§hrend der Backpropagation anpasst (Abschnitt 7/11). Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6oaKlRqVjIUO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train(D):\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 8000\n",
    "    learning_rate = 0.01\n",
    "    input_size = 13\n",
    "\n",
    "    # Daten vorbereiten\n",
    "    X = D[:, :-1].astype(np.float32)\n",
    "    labels = D[:, -1].astype(np.float32)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(labels)\n",
    "\n",
    "    # Modell definieren\n",
    "    class LogisticRegression(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(input_size, 1)  # Xw (linear layer)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            return out\n",
    "    model = LogisticRegression(input_size)\n",
    "\n",
    "    # loss (Kostenfunktion)\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Sigmoid und Binary-Cross-Entropy-Loss\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X)[:, 0]\n",
    "        cost = criterion(outputs, y)\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        cost.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # Berechnung der Accuracy\n",
    "        pred_labels = outputs > 0\n",
    "        is_correct = torch.eq(pred_labels, y.byte()).float()\n",
    "        accuracy = torch.mean(is_correct).item()\n",
    "        \n",
    "        #Damit wir einen √úberblick bekommen, wie unser Modell trainiert wird und wie sich die Genauigkeit √§ndert\n",
    "        # lassen wir uns alle 1000 Epochen die Genauigkeit ausgegeben\n",
    "        if e%1000 == 0:\n",
    "            print(\"Epoche: {:<4} | Genauigkeit: {:.3f}\".format(e+1, accuracy)) #e+1 weil es bei 0 anf√§ngt zu z√§hlen, wir aber nicht \"Epoche 0\" ausgeben wollen\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUuP-T-9jIUT",
    "outputId": "6880f3e5-64bc-4750-e7d0-744005b8b485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche: 1    | Genauigkeit: 0.596\n",
      "Epoche: 1001 | Genauigkeit: 0.880\n",
      "Epoche: 2001 | Genauigkeit: 0.883\n",
      "Epoche: 3001 | Genauigkeit: 0.880\n",
      "Epoche: 4001 | Genauigkeit: 0.878\n",
      "Epoche: 5001 | Genauigkeit: 0.880\n",
      "Epoche: 6001 | Genauigkeit: 0.866\n",
      "Epoche: 7001 | Genauigkeit: 0.880\n"
     ]
    }
   ],
   "source": [
    "# nicht f√ºr die L√∂sung im KI-Kurs notwendig\n",
    "#Trainiere das Modell\n",
    "#Lasse dir die Ver√§nderungen der Genauigkeit w√§hrend der Backpropagation ausgeben\n",
    "optimized_model = train(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT_iMNwOjIUX"
   },
   "source": [
    "Super gemacht!\n",
    "Dein Modell sollte nun eine Genauigkeit von ca. 88% erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgJAXfuIjIUZ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQzytPgujIUa"
   },
   "source": [
    "Modelle mit vielen Layern sind flexibler und k√∂nnen sich besser an die Daten anpassen. Daher ist es nun deine Aufgabe den unten folgenden Code f√ºr ein Multi-Layer-Perceptron (MLP) um die passenden Werte der Hyperparameter `n_steps` und `learning_rate` zu erweitern (Abschnitt 8/11).Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaSiBohsjIUc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train_MLP(D):\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 4000\n",
    "    learning_rate = 0.01\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "\n",
    "    # Daten vorbereiten\n",
    "    X = D[:, :-1].astype(np.float32)\n",
    "    y = D[:, -1].astype(np.float32)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X, dim=0)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 4\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Sigmoid und Binary-Cross-Entropy-Loss\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X)[:, 0]\n",
    "        cost = criterion(outputs, y)\n",
    "\n",
    "        # backward pass (berechnet die Gradienten automatisch)\n",
    "        optimizer.zero_grad()  # reset gradients (torch akkumuliert Gradienten)\n",
    "        cost.backward()  # berechnen der Gradienten\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # Berechnung der Accuracy\n",
    "        pred_labels = outputs > 0\n",
    "        is_correct = torch.eq(pred_labels, y.byte()).float()\n",
    "        accuracy = torch.mean(is_correct).item()\n",
    "\n",
    "        if e%1000 == 0:\n",
    "            print(\"Epoche: {:<4} | Genauigkeit: {:.3f}\".format(e+1, accuracy)) #e+1 weil es bei 0 anf√§ngt zu z√§hlen, wir aber nicht \"Epoche 0\" ausgeben wollen\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olPpBiDljIUh",
    "outputId": "2df9658b-98bf-4886-db3e-b750f7cc3b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche: 1    | Genauigkeit: 0.362\n",
      "Epoche: 1001 | Genauigkeit: 0.915\n",
      "Epoche: 2001 | Genauigkeit: 0.920\n",
      "Epoche: 3001 | Genauigkeit: 0.923\n"
     ]
    }
   ],
   "source": [
    "# nicht f√ºr die L√∂sung im KI-Kurs notwendig\n",
    "#Trainiere das Modell\n",
    "#Lasse dir die Ver√§nderungen der Genauigkeit w√§hrend der Backpropagation ausgeben\n",
    "optimized_model_MLP = train_MLP(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD3Q0ajdjIUl"
   },
   "source": [
    "Super - wir sehen, dass du den Einsatz eines komplexeren MLPs, die Genauigkeit steigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDzFYlYojIUm"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBU70OfJjIUo"
   },
   "source": [
    "Um zu verstehen, was Overfitting ist und wie du es erkennst, ist jetzt deine Aufgabe ein eigenes neuronales Netz zu erstellen, welches den Trainingsdatensatz perfekt (auswendig)lernt. Das hei√üt: Dein Netz sollte eine Genauigkeit von 1 auf dem Trainingsdatensatz aufweisen. Evaluiere dein Modell anschlie√üend auf den Testdaten. Dabei solltest du merken, dass dieses Modell auf den Trainingsdaten √ºberangepasst ist und die Erkenntnisse des Modells damit schlecht auf unbekannte Daten wie z.B. den Testdatensatz √ºbertragen werden k√∂nnen. In anderen Worten generalisiert das Modell schlecht (Abschnitt 9/10). Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_tW1er_jIUq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 2000\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 20\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_neurons, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOrfm--QjIUu",
    "outputId": "49432fc7-7a70-4caa-d85f-59956cc0252c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1999, Loss: 0.0000, Acc train: 1.00,Acc test: 0.41\n"
     ]
    }
   ],
   "source": [
    "# nicht f√ºr die L√∂sung im KI-Kurs notwendig\n",
    "\n",
    "#F√ºhre die Funktion train_overfitting aus und schau dir die Werte deines NNs an.\n",
    "#Evaluation: Wie gut performt das NN auf den Testdaten?\n",
    "#Spricht man hier √ºberhaupt schon von Overfitting? Recherchiere!\n",
    "optimized_model_overfitting = train(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLVkptRvjIUy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DBjfTIRjIUy"
   },
   "source": [
    "## Teste Dein Wissen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DiBAsIwjIU2"
   },
   "source": [
    "Nun hast du einiges √ºber die Methoden des maschinellen Lernens erfahren. In der letzten Aufgabe sollst du ein neuronales Netz entwickeln, welches die Hunde als krank oder gesund klassifiziert. **Achtung**: Behalte dabei immer auch Over- und Underfitting im Auge! Es gibt bei dieser Aufgabe **nicht nur eine L√∂sung**, sondern viele verschiedene L√∂sungen, die zum Ziel f√ºhren. Wir stellen dir eine dieser L√∂sungen vor (Abschnitt 10/11). Dies ist die MUSTERL√ñSUNG! Die Coding-Konsole im KI-Kurs enth√§lt weitere Informationen zu Funktionen, Argumenten oder Erkl√§rungen zum Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsIuwmeajIU2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 2000\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 20\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbpa9MjkjIU7"
   },
   "outputs": [],
   "source": [
    "#F√ºhre die Funktion train aus und schau dir die Werte deines NNs an.\n",
    "#Evaluation: Wie gut performt das NN auf den Testdaten?\n",
    "optimized_model_best = train(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUJJpCjCjIVA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j7VLzYjjIVC"
   },
   "source": [
    "Hier k√∂nnen wir sehen,\n",
    "dass wir f√ºr dieses Modell eine Trainingsgenauigkeit von 87% und eine Testgenauigkeit von 73% haben.\n",
    "Hier k√∂nnte man schon leichtes Overfitting vermuten, aber genauso gut kann man noch sagen, dass dies noch im Rahmen ist und das Modell recht gut generalisiert.\n",
    "\n",
    "Nichtsdestotrotz k√∂nnen wir uns noch ein paar weitere Beispiele von Under- sowie Overfitting ansehen.\n",
    "\n",
    "Da die Funktion `train` leider nur einen Datensatz √ºbergeben bekommt und das Modell innerhalb der Funktion definiert wird, erstellen wir uns nun zwei weitere Funktionen `train_overfit` sowie `train_underfit`, welche ein entsprechend √ºber- sowie unterangepasstes Modell produzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMx2_fu1jIVD"
   },
   "source": [
    "## EXKURS: Over- & Underfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcTx05EijIVE"
   },
   "source": [
    "Die folgenden Beispiele sind nicht mehr f√ºr die L√∂sung der Aufgaben im KI Kurs notwendig. <br> Du kannst diesen Abschnitt auch √ºberspringen und weiter unten bei **\"du hast es geschafft\"**\" fortfahren\". <br> Zum besseren Verst√§ndnis von Over- und Underfitting wollten wir dir die Infos jedoch nicht vorenthalten! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gKle9MqjIVG"
   },
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-LaFHwmjIVH"
   },
   "source": [
    "Ok, wir wollen nun also ein √ºberangepasstes Modell erzeugen.\n",
    "Schauen wir uns doch noch einmal die `train`-Funktion an und √ºberlegen, welche Parameter wir wie ab√§ndern k√∂nnen, um Overfitting zu erzeugen. √úberlege an dieser Stelle gerne kurz selbst, bevor du weiterliest.\n",
    "\n",
    "Wir haben ein paar Optionen.\n",
    "1. Wir k√∂nnen das Modell l√§nger trainieren, also `n_steps` erh√∂hen.\n",
    "2. Wir k√∂nnen das Modell komplexer gestalten, beispielsweise statt nur einer `nn.Linear`-Schicht k√∂nnen wir f√ºnf verwenden.\n",
    "\n",
    "Das probieren wir doch gleich mal aus!\n",
    "Da unser Modell nun mehr Parameter hat und wir auch mehr Trainingsiterationen durchf√ºhren, macht es Sinn, die Lernrate ein wenig herunterzuschrauben. Damit wird das Modell umso √ºberangepasster, weil wir feinere √Ñnderungen durchf√ºhren, als wenn wir eine gro√üe Lernrate verwenden w√ºrden.\n",
    "\n",
    "Unsere √Ñnderungen (vorher -> nachher) sind also:\n",
    "- `n_steps`: 2000 -> 20000\n",
    "- Schichten: 1 -> 5\n",
    "- `learning_rate`: 0.01 -> 0.003\n",
    "\n",
    "Probieren wir es doch einmal aus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHS-ZoVIjIVJ"
   },
   "outputs": [],
   "source": [
    "def train_overfit(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 20000 # <--------------- Anzahl Trainingsiterationen erh√∂ht\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "    learning_rate = 0.003 # <--------------- Lernrate verringert\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 20\n",
    "            self.layers = nn.Sequential( # <--------------- mehr Schichten hinzugef√ºgt\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size, output_size*2), # hier k√∂nnen wir uns zu Testzwecken\n",
    "                                                       # beliebige Werte f√ºr die neuen Schichten ausdenken\n",
    "                                                       # beispielsweise also output_size*2 bzw. *3\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size*2, output_size*3),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size*3, output_size*3),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size*3, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od7cUCNqjIVM",
    "outputId": "3338b3ac-0fbd-4d09-943a-de66e148ab0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19999, Loss: 0.2897, Acc train: 0.92,Acc test: 0.69\n"
     ]
    }
   ],
   "source": [
    "overfit_model = train_overfit(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Hm7lRejIVP"
   },
   "source": [
    "Hier kann man nun eindeutig das Overfitting sehen, da die Trainingsgenauigkeit deutlich √ºber der Testgenauigkeit liegt.\n",
    "Die Trainingsgenauigkeit liegt bei 0.92, w√§hrend hingegen die Testgenauigkeit bei 0.69 liegt.\n",
    "Das ist ein stolzer Unterschied von 33%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpSsPHx2jIVQ"
   },
   "source": [
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxJOrkVfjIVS"
   },
   "source": [
    "Nun wollen wir auch die andere Seite der Medaille betrachten und\n",
    "ein unterangepasstes Modell erzeugen.\n",
    "Welche Parameter k√∂nnen wir hierf√ºr ab√§ndern?\n",
    "\n",
    "Wir k√∂nnen genau das Gegenteil der vorherigen √Ñnderungen durchf√ºhren!\n",
    "Wenn die Erh√∂hung der Anzahl an Iterationen unser Netz st√§rker anpasst,\n",
    "so sollte die Verringerung der Anzahl an Iterationen unser Netz weniger stark anpassen.\n",
    "\n",
    "Bei der Lernrate sieht es ein wenig anders aus.\n",
    "Wenn wir unsere Lernrate erh√∂hen, w√ºrde dies die Verringerung der Anzahl an Trainingsiterationen ausgleichen.\n",
    "Hier erhalten wir also einen besseren Effekt, wenn wir sowohl die Lernrate als auch die Anzahl an Trainingsiterationen gering halten.\n",
    "\n",
    "Mit anderen Worten: Unser Modell trainiert nur f√ºr wenige Epochen und in jeder Epoche lernt es nur ein kleines bisschen dazu!\n",
    "\n",
    "Unser Modell k√∂nnen wir nicht noch simpler gestalten, da eine einzige Schicht bereits das Minimum ist,\n",
    "was wir f√ºr ein funktionierendes Modell ben√∂tigen.\n",
    "\n",
    "Die √Ñnderungen sind nun also (im Vergleich zur originellen `train`-Funktion):\n",
    "- `n_steps`: 2000 -> 200\n",
    "- `learning_rate`: 0.01 -> 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaQIwzPtjIVT"
   },
   "outputs": [],
   "source": [
    "def train_underfit(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    # Hyper-parameter\n",
    "    n_steps = 200 # <--------------- Anzahl Trainingsiterationen verringert\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "    learning_rate = 0.0001 # <--------------- Lernrate verringern\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            #out = out*np.nan  ---------------- muss weg\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB8aQecJjIVX",
    "outputId": "cecec78a-a8c0-40ad-e9ae-92ffaa6d639a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199, Loss: 1.8547, Acc train: 0.52,Acc test: 0.27\n"
     ]
    }
   ],
   "source": [
    "underfit_model = train_underfit(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Lug9scjIVZ"
   },
   "source": [
    "Hier sieht man nun eindeutig, dass sowohl die Trainings- als auch die Testgenauigkeit ziemlich gering sind\n",
    "Eine Genauigkeit von 50% w√§re bei unserem Datensatz mit bin√§ren Labeln (entweder 0 oder 1) genauso gut wie raten, also macht unser Modell gerade keine allzu gute Vorhersagen.\n",
    "\n",
    "Wenn ihr den Code oben einige Male ausf√ºhrt, werdet ihr merken, dass die Testgenauigkeit ein gutes St√ºck schwankt.\n",
    "Dies liegt daran, dass unser Modell nicht wirklich etwas lernt und damit recht \"unsinnige\" Vorhersagen macht.\n",
    "Manchmal sind diese sehr gut, manchmal sehr schlecht. Unser Netz ist damit also nicht nur underfit, sondern auch noch overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqZp58MrjIVi"
   },
   "source": [
    "**Grafische Analyse von Over- & Underfitting**\n",
    "    \n",
    "Um Over- und Underfitting noch besser zu visualisieren, k√∂nnt ihr euch den grafischen Verlauf der Genauigkeiten / des Losses zur√ºckgeben lassen und diesen plotten. Hierf√ºr legen wir zwei neue Variablen `train_acc_hist` und `test_acc_hist` die jeweils den Verlauf der Trainings- sowie Testgenauigkeit beinhalten werden. Nach jeder Epoche f√ºgen wir die aktuellen Genauigkeiten in diese beiden Listen ein. Am Ende lassen wir uns diese Listen ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCfINSacjIVj"
   },
   "outputs": [],
   "source": [
    "def train_overfit_with_hist(D):\n",
    "\n",
    "    train, test = D[:200], D[200:]  # Die Auswertung der Aufgabe basiert auf diesem Split\n",
    "\n",
    "    train_acc_hist = [] # --------------------------------------------------------- NEU\n",
    "    test_acc_hist = [] # --------------------------------------------------------- NEU\n",
    "    \n",
    "    # Hyper-parameter\n",
    "    n_steps = 20000\n",
    "    input_size = 13\n",
    "    output_size = 1\n",
    "    learning_rate = 0.003\n",
    "\n",
    "    # Trainings-Daten vorbereiten\n",
    "    X = train[:, :-1].astype(np.float32)\n",
    "    y = train[:, -1].astype(np.float32)\n",
    "    X_train = torch.from_numpy(X)\n",
    "    y_train = torch.from_numpy(y)\n",
    "    feature_means = torch.mean(X_train, dim=0)\n",
    "\n",
    "    # Test-Daten vorbereiten\n",
    "    X = test[:, :-1].astype(np.float32)\n",
    "    y = test[:, -1].astype(np.float32)\n",
    "    X_test = torch.from_numpy(X)\n",
    "    y_test = torch.from_numpy(y)\n",
    "\n",
    "    # Modell definieren\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            n_neurons = 20\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size, output_size*2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size*2, output_size*3),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size*3, output_size*3),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(output_size*3, output_size)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x - feature_means\n",
    "            out = self.layers(x)\n",
    "            return out\n",
    "\n",
    "    model = MLP(input_size)\n",
    "\n",
    "    # loss and optimizer\n",
    "    # checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()  # sigmoid + binary cross entropy\n",
    "\n",
    "    # optimizer\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Alternative zu Adam\n",
    "    # Dokumentation: https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "    # momentum = 0.9  # Wert zwischen 0. und 1.\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # trainieren des Modells\n",
    "    for e in range(n_steps):\n",
    "        # forward pass\n",
    "        outputs = model.forward(X_train)[:, 0]  # Xw (linear layer)\n",
    "        loss = criterion(outputs, y_train)  # sigmoid and cross-entropy loss\n",
    "\n",
    "        # backward pass (automatically computes gradients)\n",
    "        optimizer.zero_grad()  # reset gradients (torch accumulates them)\n",
    "        loss.backward()  # computes gradients\n",
    "\n",
    "        # Optimierungsschritt durchfuehren\n",
    "        optimizer.step()\n",
    "\n",
    "        # berechne Trainings-Accuracy\n",
    "        outputs = model.forward(X_train)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_train.byte()).float()\n",
    "        accuracy_train = torch.mean(is_correct).item()\n",
    "        \n",
    "        train_acc_hist.append(accuracy_train) # --------------------------------------------------------- NEU\n",
    "\n",
    "        # berechne Test-Accuracy\n",
    "        outputs = model.forward(X_test)[:, 0]\n",
    "        pred_y = outputs > 0\n",
    "        is_correct = torch.eq(pred_y, y_test.byte()).float()\n",
    "        accuracy_test = torch.mean(is_correct).item()\n",
    "        \n",
    "        test_acc_hist.append(accuracy_test) # --------------------------------------------------------- NEU\n",
    "\n",
    "    print(f'Epoch {e}, Loss: {loss:.4f}, Acc train: {accuracy_train:.2f},' \\\n",
    "          f'Acc test: {accuracy_test:.2f}')\n",
    "    return model, train_acc_hist, test_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQpv77YbjIVp",
    "outputId": "6d6c505c-c771-467c-ca2e-a8e452037a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19999, Loss: 0.3478, Acc train: 0.85,Acc test: 0.54\n"
     ]
    }
   ],
   "source": [
    "overfit_model, train_acc_hist_overfit, test_acc_hist_overfit = train_overfit_with_hist(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPOFPe4PjIVs"
   },
   "source": [
    "Hier haben wir also ein √ºberangepasstes Modell erzeugt.\n",
    "Visualisieren wir doch einmal den Verlauf der Trainings- sowie Testgenauigkeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0auD9tTjIVu"
   },
   "outputs": [],
   "source": [
    "# matplotlib ist eine Bibliothek, die das Erstellen von Visualisierungen erm√∂glicht\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkDGRG2ejIVw",
    "outputId": "f3b779cb-9ad9-41c1-f5e6-453e93bb55d3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(20000), train_acc_hist_overfit, label=\"train\") #20000 ist die Anzahl an Trainingsiterationen\n",
    "plt.plot(np.arange(20000), test_acc_hist_overfit, label=\"test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us6XdEIQjIVy"
   },
   "source": [
    "Aha! Hier sehen wir also, wie die Trainingsgenauigkeit in die H√∂he schie√üt, die Testgenauigkeit aber auf einem deutlich tieferen Wert sitzen bleibt und nicht weiter nach oben geht.\n",
    "\n",
    "Dies war jetzt ein Beispiel, an welchem man Overfitting erkennen kann.\n",
    "Die Kurven f√ºr ein unterangepasstes, sowie ein normales Modell k√∂nnten wir hier auch noch plotten,\n",
    "aber diese sehen nicht ganz so spektakul√§r aus und sind auch nicht ganz so wichtig.\n",
    "Gerne kannst du einmal selbst versuchen, diese Kurven zu plotten!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1SaYQZ2jIVz"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPizd-eSjIV2"
   },
   "source": [
    "### Du hast es geschafft! ü•≥ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHdIWXp5jIV3"
   },
   "source": [
    "Du kannst wirklich stolz auf dich sein! Es ist sicherlich einiges an Zeit und Hirnschmalz in dieses Tutorial geflossen. Vielleicht hat dich dieses Tutorial ja dazu inspiriert eine eigene Idee mit den Methoden der KI anzugehen. Reiche dein Projekt j√§hrlich beim Bundeswettbewerb KI ein und gewinne tolle Preise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF0rqGJTjIV4"
   },
   "source": [
    "Wie geht es jetzt weiter? Es warten weitere spannende Kurselemente auf euch:\n",
    "- KI & Gesselschaft, spannende Artikel & Quizfragen\n",
    "- Knackt die √úberflieger Aufgaben des Python Grundkurses\n",
    "- √úbt im Aufgabenpool\n",
    "- Macht das n√§chste Tutorial zu CNNs - das Recycling Tutorial mit Boris\n",
    "\n",
    "Es gibt also viel zu tun!\n",
    "\n",
    "Unter dem Reiter --> Zertifikat im Hauptmen√º kannst du dir deine Leistungen in diesem Kurs bescheinigen lassen.\n",
    "\n",
    "Wir w√ºnschen dir viel Spa√ü beim Lernen üòä\n",
    "Euer BWKI Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7oZaBGSjIV5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Musterlosung_Kapitel_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
